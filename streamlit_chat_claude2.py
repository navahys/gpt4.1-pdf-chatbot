import os
import json
import glob
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

# Page configuration
st.set_page_config(
    page_title="ì‚°ì—…ì¬í•´ ë²•ë¥  ì •ë³´ ì•ˆë‚´ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“š",
    layout="wide"
)

# Constants
LOG_PATH = "anonymized_chat_log.json"  # ìµëª…í™”ëœ ë¡œê·¸ ì €ì¥
MODEL_NAME = "gpt-4.1"
TEMP_DIR = "temp_pdfs"
DATA_DIR = "data"
VECTOR_STORE_PATH = "vectorstore"

# Create required directories if they don't exist
for dir_path in [TEMP_DIR, DATA_DIR, VECTOR_STORE_PATH]:
    os.makedirs(dir_path, exist_ok=True)

# Helper functions
def load_api_key():
    """Load OpenAI API key from .env file or session state"""
    load_dotenv()
    
    if "openai_api_key" in st.session_state and st.session_state.openai_api_key:
        return st.session_state.openai_api_key
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    return api_key

def anonymize_chat_data(chat_data):
    """ê°œì¸ì •ë³´ ë° ë¯¼ê°ì •ë³´ ìµëª…í™” ì²˜ë¦¬"""
    # ì‹¤ì œ êµ¬í˜„ì‹œ ê°œì¸ì‹ë³„ì •ë³´ íŒ¨í„´ í•„í„°ë§ ë¡œì§ ì¶”ê°€
    # ì˜ˆ: ì´ë¦„, ì£¼ë¯¼ë²ˆí˜¸, ì „í™”ë²ˆí˜¸, ì£¼ì†Œ ë“± íŒ¨í„´ ë§ˆìŠ¤í‚¹
    return chat_data

def load_chat_history():
    """Load previous chat history from log file"""
    try:
        if os.path.exists(LOG_PATH):
            with open(LOG_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        st.error(f"ìµëª…í™”ëœ ì°¸ê³  ìë£Œë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    return []

def save_chat_history(history):
    """Save anonymized chat history to log file"""
    try:
        # ì‚¬ìš©ì ë™ì˜ ì—¬ë¶€ í™•ì¸
        if st.session_state.get("data_consent", False):
            anonymized_history = anonymize_chat_data(history)
            with open(LOG_PATH, "w", encoding="utf-8") as f:
                json.dump(anonymized_history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ìµëª…í™”ëœ ì°¸ê³  ìë£Œë¥¼ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def load_default_pdfs():
    """Load PDFs from the data directory"""
    pdf_files = glob.glob(os.path.join(DATA_DIR, "*.pdf"))
    
    if not pdf_files:
        st.warning(f"'{DATA_DIR}' ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    with st.spinner(f"{len(pdf_files)}ê°œì˜ ê¸°ë³¸ ë²•ë¥  ì°¸ê³ ìë£Œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        processed_files = []
        
        for pdf_path in pdf_files:
            try:
                # Try with PyPDFLoader first
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                
                # If empty, try with UnstructuredPDFLoader
                if not documents:
                    loader = UnstructuredPDFLoader(pdf_path)
                    documents = loader.load()
                
                all_docs.extend(documents)
                processed_files.append(os.path.basename(pdf_path))
                st.session_state.default_pdfs.append(os.path.basename(pdf_path))
            except Exception as e:
                st.error(f"'{os.path.basename(pdf_path)}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if not all_docs:
            st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=300,
            separators=["\n\n", "\n", ".", " ", ""],
        )
        texts = text_splitter.split_documents(all_docs)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(api_key=load_api_key())
        vectordb = FAISS.from_documents(texts, embeddings)
        
        # Save the vectorstore for future use
        vectordb.save_local(VECTOR_STORE_PATH)
        
        st.success(f"âœ… {len(processed_files)}ê°œì˜ ë²•ë¥  ì°¸ê³ ìë£Œ ì²˜ë¦¬ ì™„ë£Œ")
        return vectordb

def load_saved_vectorstore():
    """Load saved vector store if it exists"""
    if os.path.exists(VECTOR_STORE_PATH) and os.path.isdir(VECTOR_STORE_PATH):
        try:
            embeddings = OpenAIEmbeddings(api_key=load_api_key())
            vectordb = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
            return vectordb
        except Exception as e:
            st.error(f"ì €ì¥ëœ ì°¸ê³ ìë£Œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    return None

def process_pdfs(uploaded_files):
    """Process uploaded PDF files and merge with existing vector store"""
    with st.spinner("ë²•ë¥  ì°¸ê³ ìë£Œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        for uploaded_file in uploaded_files:
            # Save uploaded file to temp directory
            file_path = os.path.join(TEMP_DIR, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            try:
                # Try with PyPDFLoader first
                loader = PyPDFLoader(file_path)
                documents = loader.load()
                
                # If empty, try with UnstructuredPDFLoader
                if not documents:
                    loader = UnstructuredPDFLoader(file_path)
                    documents = loader.load()
                
                all_docs.extend(documents)
                st.session_state.processed_files.append(uploaded_file.name)
            except Exception as e:
                st.error(f"'{uploaded_file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if not all_docs:
            st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return st.session_state.vectordb  # Return existing vectordb
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""],
        )
        texts = text_splitter.split_documents(all_docs)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(api_key=load_api_key())
        
        # If we already have a vectordb, merge the new documents
        if st.session_state.vectordb:
            vectordb = st.session_state.vectordb
            vectordb.add_documents(texts)
        else:
            vectordb = FAISS.from_documents(texts, embeddings)
        
        # Save the updated vectorstore
        vectordb.save_local(VECTOR_STORE_PATH)
        
        return vectordb

def initialize_chain(vectordb, doc_weight=0.7, model_temperature=0.7):
    """Initialize the conversation chain with the vector store and balance settings"""
    api_key = load_api_key()
    if not api_key:
        return None
    
    llm = ChatOpenAI(
        model=MODEL_NAME, 
        temperature=model_temperature,
        api_key=api_key
    )
    
    # Rewrite prompt for question improvement
    rewrite_prompt = PromptTemplate(
        input_variables=["question"],
        template="""
        ë‹¹ì‹ ì€ ì‚°ì—…ì¬í•´ ê´€ë ¨ ë²•ë¥  ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” AIì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ í•˜ì—¬ ì ì ˆí•œ ë²•ë¥  ì°¸ê³ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ì„¸ìš”.
        
        ì›ë³¸ ì§ˆë¬¸: {question}
        
        ë²•ë¥ ì  ë§¥ë½ê³¼ ê´€ë ¨ ì¡°í•­ì„ ì°¾ê¸° ìœ„í•´ ê°œì„ ëœ ì§ˆë¬¸:
        """
    )   
    
    # QA prompt to include source documents with fixed doc_weight
    qa_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=f"""
        ë‹¹ì‹ ì€ ì‚°ì—…ì¬í•´ ê´€ë ¨ ë²•ë¥  ë¬¸ì„œì™€ ë²•ì  ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ë²•ë¥  ì •ë³´ ì•ˆë‚´ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

        ## ë²•ë¥  ì°¸ê³ ìë£Œ ë‚´ìš©:
        {{context}}

        ## ì§ˆë¬¸:
        {{question}}

        ## ë‹µë³€ ìƒì„± ì§€ì¹¨:
        - ë¬¸ì„œ ë‚´ìš© ì˜ì¡´ë„: {doc_weight*9.5}/10 (10ì´ë©´ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©, 1ì´ë©´ AI ì§€ì‹ ê±°ì˜ ì „ì ìœ¼ë¡œ ì‚¬ìš©)
        - ì‚°ì—…ì•ˆì „ë³´ê±´ë²•, ì‚°ì—…ì¬í•´ë³´ìƒë³´í—˜ë²• ë“± ê´€ë ¨ ë²•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”
        - ë²•ì¡°í•­ê³¼ ê´€ë ¨ íŒë¡€ê°€ ìˆë‹¤ë©´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš” (ì˜ˆ: "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì œ00ì¡°ì— ë”°ë¥´ë©´...")
        - ì‚¬ìš©ìê°€ ë²•ì  ì ˆì°¨, ê¶Œë¦¬, ì˜ë¬´ì— ëŒ€í•´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
        - ë³µì¡í•œ ë²•ë¥  ìš©ì–´ê°€ ìˆë‹¤ë©´ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
        - ë¬¸ì„œì— íŠ¹ì • ìƒí™©ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ë‹¤ë©´ "êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë²•ë¥  ì „ë¬¸ê°€ì™€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤"ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”
        - "ë²•ë¥  ìë¬¸"ì´ë‚˜ "ë²•ë¥  ìƒë‹´"ì´ë¼ëŠ” í‘œí˜„ ëŒ€ì‹  "ë²•ë¥  ì •ë³´ ì œê³µ", "ë²•ë¥  ì°¸ê³ ìë£Œ ì•ˆë‚´" ë“±ì˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”
        - ë‹µë³€ ë§ˆì§€ë§‰ì— í•­ìƒ ë‹¤ìŒ ë©´ì±… ì¡°í•­ì„ í¬í•¨í•˜ì„¸ìš”: 

        "â€» ì•ˆë‚´ ì‚¬í•­: ì œê³µí•´ ë“œë¦° ì •ë³´ëŠ” ì¼ë°˜ì ì¸ ë²•ë¥  ì°¸ê³ ìë£Œë¡œ, ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©° ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ë³€í˜¸ì‚¬ê°€ ì•„ë‹ˆë©°, ë²•ë¥  ìë¬¸ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì‚°ì¬ ì „ë¬¸ ë³€í˜¸ì‚¬ë‚˜ ê´€ë ¨ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        
        ë‹µë³€:
        """
    )
    
    # Set up memory and conversation chain
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    # Improved chain with source documents
    rewrite_chain = LLMChain(llm=llm, prompt=rewrite_prompt)
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 7}),
        memory=memory,
        condense_question_prompt=rewrite_prompt,
        combine_docs_chain_kwargs={"prompt": qa_prompt}
    )
    
    st.session_state.rewrite_chain = rewrite_chain
    return conversation_chain

# ë¯¼ê°ì •ë³´ ì…ë ¥ í•„í„°ë§ í•¨ìˆ˜
def contains_sensitive_info(text):
    """ë¯¼ê°ì •ë³´ íŒ¨í„´ ê²€ì‚¬"""
    import re
    # ì£¼ë¯¼ë²ˆí˜¸ íŒ¨í„´
    korean_id_pattern = r'\d{6}[-]\d{7}'
    # ì „í™”ë²ˆí˜¸ íŒ¨í„´
    phone_pattern = r'01[016789][-\s]?\d{3,4}[-\s]?\d{4}'
    
    if re.search(korean_id_pattern, text) or re.search(phone_pattern, text):
        return True
    return False

# Initialize session state variables
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "conversation_chain" not in st.session_state:
    st.session_state.conversation_chain = None

if "rewrite_chain" not in st.session_state:
    st.session_state.rewrite_chain = None

if "processed_files" not in st.session_state:
    st.session_state.processed_files = []

if "default_pdfs" not in st.session_state:
    st.session_state.default_pdfs = []

if "openai_api_key" not in st.session_state:
    st.session_state.openai_api_key = ""

if "doc_weight" not in st.session_state:
    st.session_state.doc_weight = 7

if "model_temperature" not in st.session_state:
    st.session_state.model_temperature = 0.7

if "data_consent" not in st.session_state:
    st.session_state.data_consent = False

if "terms_accepted" not in st.session_state:
    st.session_state.terms_accepted = False

if "vectordb" not in st.session_state:
    # Try to load existing vectorstore first
    st.session_state.vectordb = load_saved_vectorstore()
    
    # If no existing vectorstore, load default PDFs
    if st.session_state.vectordb is None:
        st.session_state.vectordb = load_default_pdfs()
        
    # Initialize conversation chain if vectordb exists
    if st.session_state.vectordb:
        st.session_state.conversation_chain = initialize_chain(
            st.session_state.vectordb,
            st.session_state.doc_weight / 10,
            st.session_state.model_temperature
        )

# UI Layout
st.title("ğŸ“„ ì‚°ì—…ì¬í•´ ë²•ë¥  ì •ë³´ ì•ˆë‚´ ì‹œìŠ¤í…œ")
st.markdown("ì´ ì‹œìŠ¤í…œì€ ë²•ë¥  ìë¬¸ì„ ì œê³µí•˜ì§€ ì•Šìœ¼ë©°, ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ì™€ ì°¸ê³ ìë£Œë§Œ ì•ˆë‚´í•©ë‹ˆë‹¤.")

# ì‚¬ìš©ì ë™ì˜ ë° ì´ìš©ì•½ê´€ ì²´í¬
if not st.session_state.terms_accepted:
    st.warning("âš ï¸ ì‹œìŠ¤í…œì„ ì´ìš©í•˜ê¸° ì „ì— ì•„ë˜ ì´ìš©ì•½ê´€ê³¼ ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨ì— ë™ì˜í•´ì£¼ì„¸ìš”.")
    
    with st.expander("ğŸ“œ ì´ìš©ì•½ê´€ (í•„ìˆ˜)", expanded=True):
        st.markdown("""
        ## ì´ìš©ì•½ê´€
        
        1. **ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì **: ë³¸ ì‹œìŠ¤í…œì€ ë²•ë¥  ìë¬¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ìœ¼ë¡œ ìš´ì˜ë©ë‹ˆë‹¤.
        2. **ë²•ì  ì±…ì„ ì œí•œ**: ì œê³µë˜ëŠ” ëª¨ë“  ì •ë³´ëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©°, ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
        3. **ì •ë³´ì˜ ì •í™•ì„±**: ìµœì‹  ë²•ë ¹ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë…¸ë ¥í•˜ë‚˜, ì‹¤ì œ ë²•ë ¹ê³¼ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        4. **ì „ë¬¸ê°€ ìƒë‹´ ê¶Œì¥**: êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
        5. **ìˆ˜ìµ ëª¨ë¸**: ë³¸ ì‹œìŠ¤í…œì€ êµìœ¡ ë° í•™ìŠµ ëª©ì ìœ¼ë¡œ ì œê³µë˜ë©°, ìœ ë£Œ ë²•ë¥  ìë¬¸ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """)
    
    with st.expander("ğŸ”’ ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨ (í•„ìˆ˜)", expanded=True):
        st.markdown("""
        ## ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨
        
        1. **ê°œì¸ì •ë³´ ìˆ˜ì§‘ í•­ëª©**: ëŒ€í™” ë‚´ìš© ì¤‘ ë²•ë¥  ì •ë³´ ì œê³µì— í•„ìš”í•œ ìµœì†Œí•œì˜ ì •ë³´ë§Œ ìµëª…í™”í•˜ì—¬ ì €ì¥ë©ë‹ˆë‹¤.
        2. **ë¯¼ê°ì •ë³´ ë³´í˜¸**: ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“± ë¯¼ê°ì •ë³´ëŠ” ì…ë ¥í•˜ì§€ ë§ˆì‹œê³ , ìë™ í•„í„°ë§ë©ë‹ˆë‹¤.
        3. **ë°ì´í„° ì €ì¥ ë° í™œìš©**: ë™ì˜ ì‹œ ìµëª…í™”ëœ ëŒ€í™” ë‚´ìš©ì„ ì‹œìŠ¤í…œ ê°œì„  ëª©ì ìœ¼ë¡œë§Œ í™œìš©í•©ë‹ˆë‹¤.
        4. **ì œ3ì ì œê³µ**: ìˆ˜ì§‘ëœ ì •ë³´ëŠ” ì œ3ìì—ê²Œ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        5. **ë°ì´í„° ë³´ê´€ ê¸°ê°„**: ìˆ˜ì§‘ëœ ì •ë³´ëŠ” 30ì¼ê°„ ë³´ê´€ í›„ ìë™ ì‚­ì œë©ë‹ˆë‹¤.
        """)
    
    col1, col2 = st.columns(2)
    with col1:
        terms_check = st.checkbox("ì´ìš©ì•½ê´€ì— ë™ì˜í•©ë‹ˆë‹¤ (í•„ìˆ˜)")
    with col2:
        privacy_check = st.checkbox("ê°œì¸ì •ë³´ ì²˜ë¦¬ë°©ì¹¨ì— ë™ì˜í•©ë‹ˆë‹¤ (í•„ìˆ˜)")
    
    data_consent = st.checkbox("ëŒ€í™” ë‚´ìš©ì„ ìµëª…í™”í•˜ì—¬ ì‹œìŠ¤í…œ ê°œì„ ì— í™œìš©í•˜ëŠ” ê²ƒì— ë™ì˜í•©ë‹ˆë‹¤ (ì„ íƒ)")
    
    if st.button("ë™ì˜í•˜ê³  ì‹œì‘í•˜ê¸°", disabled=not (terms_check and privacy_check)):
        st.session_state.terms_accepted = True
        st.session_state.data_consent = data_consent
        st.rerun()
    
    st.stop()  # ë™ì˜ ì „ê¹Œì§€ ì•„ë˜ ë‚´ìš© í‘œì‹œí•˜ì§€ ì•ŠìŒ

# Sidebar for API Key and settings
with st.sidebar:
    st.header("ì„¤ì •")
    
    # API Key input
    api_key_input = st.text_input(
        "OpenAI API í‚¤",
        type="password",
        value=st.session_state.openai_api_key,
        help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. .env íŒŒì¼ì— ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
    )
    
    if api_key_input:
        st.session_state.openai_api_key = api_key_input
    
    # Show loaded default files
    if st.session_state.default_pdfs:
        st.subheader("ê¸°ë³¸ ë¡œë“œëœ ë²•ë¥  ì°¸ê³ ìë£Œ")
        for file in st.session_state.default_pdfs:
            st.write(f"- {file}")
    
    # Additional file upload
    st.subheader("ì¶”ê°€ ë²•ë¥  ì°¸ê³ ìë£Œ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type="pdf",
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸ“„ ì¶”ê°€ ì°¸ê³ ìë£Œ ì²˜ë¦¬í•˜ê¸°"):
        st.session_state.vectordb = process_pdfs(uploaded_files)
        if st.session_state.vectordb:
            st.session_state.conversation_chain = initialize_chain(
                st.session_state.vectordb, 
                st.session_state.doc_weight / 10,
                st.session_state.model_temperature
            )
            st.success(f"âœ… {len(uploaded_files)}ê°œ ì¶”ê°€ ì°¸ê³ ìë£Œ ì²˜ë¦¬ ì™„ë£Œ")
    
    # Show processed user files
    if st.session_state.processed_files:
        st.subheader("ì‚¬ìš©ì ì¶”ê°€ ì°¸ê³ ìë£Œ")
        for file in st.session_state.processed_files:
            st.write(f"- {file}")
    
    # ë°ì´í„° ê´€ë¦¬ ì˜µì…˜
    st.divider()
    st.subheader("ë°ì´í„° ê´€ë¦¬")
    
    if st.button("ğŸ”„ ê¸°ë³¸ ì°¸ê³ ìë£Œ ì¬ì²˜ë¦¬"):
        st.session_state.default_pdfs = []
        st.session_state.vectordb = load_default_pdfs()
        if st.session_state.vectordb:
            st.session_state.conversation_chain = initialize_chain(
                st.session_state.vectordb, 
                st.session_state.doc_weight / 10,
                st.session_state.model_temperature
            )
    
    if st.button("ğŸ—‘ï¸ ì°¸ê³ ìë£Œ ì´ˆê¸°í™”", type="primary"):
        if os.path.exists(VECTOR_STORE_PATH):
            import shutil
            shutil.rmtree(VECTOR_STORE_PATH)
            os.makedirs(VECTOR_STORE_PATH, exist_ok=True)
        st.session_state.vectordb = None
        st.session_state.conversation_chain = None
        st.session_state.default_pdfs = []
        st.session_state.processed_files = []
        st.success("âœ… ì°¸ê³ ìë£Œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
    
    # Response balance settings
    st.divider()
    st.subheader("ì •ë³´ ì œê³µ ì„¤ì •")
    
    # Document weight slider
    doc_weight = st.slider(
        "ì°¸ê³ ìë£Œ ì˜ì¡´ë„",
        min_value=1,
        max_value=10,
        value=st.session_state.doc_weight,
        help="10ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì°¸ê³ ìë£Œ ë‚´ìš©ì— ì¶©ì‹¤í•˜ê²Œ, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ AIì˜ ì§€ì‹ì„ ë” í™œìš©í•©ë‹ˆë‹¤."
    )
    
    # Model temperature slider
    model_temp = st.slider(
        "ì‘ë‹µ ë‹¤ì–‘ì„±",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.model_temperature,
        step=0.1,
        help="ë†’ì„ìˆ˜ë¡ ë” ë‹¤ì–‘í•œ ë‹µë³€, ë‚®ì„ìˆ˜ë¡ ë” ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # Apply settings button
    if st.button("ì„¤ì • ì ìš©"):
        if doc_weight != st.session_state.doc_weight or model_temp != st.session_state.model_temperature:
            st.session_state.doc_weight = doc_weight
            st.session_state.model_temperature = model_temp
            
            # Reinitialize chain with new settings if vectordb exists
            if st.session_state.vectordb:
                st.session_state.conversation_chain = initialize_chain(
                    st.session_state.vectordb,
                    doc_weight / 10,
                    model_temp
                )
                st.success("âœ… ìƒˆë¡œìš´ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # Settings info
    current_settings = f"""
    **í˜„ì¬ ì„¤ì •:**
    - ì°¸ê³ ìë£Œ ì˜ì¡´ë„: {st.session_state.doc_weight}/10
    - ì‘ë‹µ ë‹¤ì–‘ì„±: {st.session_state.model_temperature:.1f}
    """
    st.markdown(current_settings)
    
    # Consent settings
    st.divider()
    st.subheader("ê°œì¸ì •ë³´ ë™ì˜ ì„¤ì •")
    
    data_consent = st.checkbox("ìµëª…í™”ëœ ëŒ€í™” ì €ì¥ì— ë™ì˜", 
                               value=st.session_state.data_consent,
                               help="ì‹œìŠ¤í…œ ê°œì„ ì„ ìœ„í•´ ìµëª…í™”ëœ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•©ë‹ˆë‹¤.")
    
    if data_consent != st.session_state.data_consent:
        st.session_state.data_consent = data_consent
        st.success("âœ… ê°œì¸ì •ë³´ ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # History management
    st.divider()
    st.subheader("ëŒ€í™” ê¸°ë¡ ê´€ë¦¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ’¾ ëŒ€í™” ì €ì¥"):
            if st.session_state.data_consent:
                save_chat_history(st.session_state.chat_history)
                st.success("âœ… ìµëª…í™”ëœ ëŒ€í™” ì €ì¥ ì™„ë£Œ")
            else:
                st.warning("âš ï¸ ëŒ€í™” ì €ì¥ì„ ìœ„í•´ì„œëŠ” ë™ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    with col2:
        if st.button("ğŸ“‚ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°"):
            st.session_state.chat_history = load_chat_history()
            st.success("âœ… ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.success("âœ… ëŒ€í™” ë‚´ì—­ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

# Main chat area
st.markdown("""
### âš ï¸ ë²•ì  ì±…ì„ ì œí•œ ì•ˆë‚´
ì´ ì‹œìŠ¤í…œì€ ë²•ë¥  ìë¬¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ë§Œ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“  ì •ë³´ëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©° ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë²•ë¥  ë¬¸ì œëŠ” ë°˜ë“œì‹œ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì„¸ìš”.
""")

if not st.session_state.vectordb:
    st.warning("âš ï¸ 'data' ë””ë ‰í† ë¦¬ì— ê¸°ë³¸ ì°¸ê³ ìë£Œê°€ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.")
elif not st.session_state.conversation_chain:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'ì°¸ê³ ìë£Œ ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.")
else:
    st.success(f"âœ… {len(st.session_state.default_pdfs) + len(st.session_state.processed_files)}ê°œì˜ ë²•ë¥  ì°¸ê³ ìë£Œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ë¯¼ê°ì •ë³´ ì…ë ¥ ì£¼ì˜ì‚¬í•­ í‘œì‹œ
st.markdown("""
#### âš ï¸ ë¯¼ê°ì •ë³´ ì…ë ¥ ì£¼ì˜
- ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ ë“± ê°œì¸ ì‹ë³„ ì •ë³´ë¥¼ ì…ë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- ëª¨ë“  ëŒ€í™”ëŠ” ìµëª…í™”ë˜ì–´ ì €ì¥ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ë™ì˜í•œ ê²½ìš°).
- ì‹¤ì œ ì‚¬ê±´ì˜ êµ¬ì²´ì ì¸ ì •í™©ë³´ë‹¤ëŠ” ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ì§ˆë¬¸í•˜ì„¸ìš”.
""")

# Display chat history
chat_container = st.container()
with chat_container:
    for user_msg, bot_msg in st.session_state.chat_history:
        with st.chat_message("user"):
            st.markdown(user_msg)
        with st.chat_message("assistant"):
            st.markdown(bot_msg)

# User input
if st.session_state.conversation_chain:
    user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    if user_question:
        # ë¯¼ê°ì •ë³´ í•„í„°ë§
        if contains_sensitive_info(user_question):
            with st.chat_message("assistant"):
                st.error("""
                âš ï¸ ê°œì¸ì •ë³´ ë³´í˜¸ ì•Œë¦¼
                
                ì…ë ¥í•˜ì‹  ë‚´ìš©ì— ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì „í™”ë²ˆí˜¸ì™€ ê°™ì€ ë¯¼ê°ì •ë³´ê°€ í¬í•¨ëœ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤. 
                ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ ë¯¼ê°ì •ë³´ë¥¼ ì œì™¸í•˜ê³  ì§ˆë¬¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
                """)
        else:
            # Display user message immediately
            with st.chat_message("user"):
                st.markdown(user_question)
            
            # Process the question and get a response
            with st.spinner("ë²•ë¥  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                try:
                    # Improve the question
                    improved_question = st.session_state.rewrite_chain.run(user_question)
                    
                    # Get response with improved question and current document weight
                    response = st.session_state.conversation_chain.run(improved_question)
                    
                    # Display assistant response
                    with st.chat_message("assistant"):
                        st.markdown(response)
                    
                    # Add to history
                    st.session_state.chat_history.append((user_question, response))
                    
                    # Auto-save history if consent given
                    if st.session_state.data_consent:
                        save_chat_history(st.session_state.chat_history)
                    
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    st.error("ìƒì„¸ ì •ë³´: " + str(type(e).__name__) + " - " + str(e))

# Footer with
