import os
import json
import glob
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain, LLMChain
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory

# Page configuration
st.set_page_config(
    page_title="PDF RAG with GPT-4.1",
    page_icon="ğŸ“š",
    layout="wide"
)

# Constants
LOG_PATH = "chat_log.json"
MODEL_NAME = "gpt-4.1"
TEMP_DIR = "temp_pdfs"
DATA_DIR = "data"  # ê¸°ë³¸ PDF íŒŒì¼ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
VECTOR_STORE_PATH = "vectorstore"  # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ

# Create required directories if they don't exist
for dir_path in [TEMP_DIR, DATA_DIR, VECTOR_STORE_PATH]:
    os.makedirs(dir_path, exist_ok=True)

# Helper functions
def load_api_key():
    """Load OpenAI API key from .env file or session state"""
    load_dotenv()
    
    if "openai_api_key" in st.session_state and st.session_state.openai_api_key:
        return st.session_state.openai_api_key
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    return api_key

def load_chat_history():
    """Load previous chat history from log file"""
    try:
        if os.path.exists(LOG_PATH):
            with open(LOG_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        st.error(f"ì±„íŒ… ê¸°ë¡ì„ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    return []

def save_chat_history(history):
    """Save chat history to log file"""
    try:
        with open(LOG_PATH, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def load_default_pdfs():
    """Load PDFs from the data directory"""
    pdf_files = glob.glob(os.path.join(DATA_DIR, "*.pdf"))
    
    if not pdf_files:
        st.warning(f"'{DATA_DIR}' ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    with st.spinner(f"{len(pdf_files)}ê°œì˜ ê¸°ë³¸ PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        processed_files = []
        
        for pdf_path in pdf_files:
            try:
                # Try with PyPDFLoader first
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                
                # If empty, try with UnstructuredPDFLoader
                if not documents:
                    loader = UnstructuredPDFLoader(pdf_path)
                    documents = loader.load()
                
                all_docs.extend(documents)
                processed_files.append(os.path.basename(pdf_path))
                st.session_state.default_pdfs.append(os.path.basename(pdf_path))
            except Exception as e:
                st.error(f"'{os.path.basename(pdf_path)}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if not all_docs:
            st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # ë²•ë¥  ë¬¸ì„œì˜ ë§¥ë½ì„ ë” ì˜ ìœ ì§€í•˜ê¸° ìœ„í•´ chunk_size ì¦ê°€
            chunk_overlap=300,  # ë” ë§ì€ ì¤‘ë³µìœ¼ë¡œ ë²•ë¥  ë¬¸ë§¥ ìœ ì§€
            separators=["\n\n", "\n", ".", " ", ""],  # ë²•ë¥  ë¬¸ì„œì˜ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í• 
        )
        texts = text_splitter.split_documents(all_docs)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(api_key=load_api_key())
        vectordb = FAISS.from_documents(texts, embeddings)
        
        # Save the vectorstore for future use
        vectordb.save_local(VECTOR_STORE_PATH)
        
        st.success(f"âœ… {len(processed_files)}ê°œì˜ ê¸°ë³¸ PDF íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
        return vectordb

def load_saved_vectorstore():
    """Load saved vector store if it exists"""
    if os.path.exists(VECTOR_STORE_PATH) and os.path.isdir(VECTOR_STORE_PATH):
        try:
            embeddings = OpenAIEmbeddings(api_key=load_api_key())
            vectordb = FAISS.load_local(VECTOR_STORE_PATH, embeddings,allow_dangerous_deserialization=True)
            return vectordb
        except Exception as e:
            st.error(f"ì €ì¥ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    return None

def process_pdfs(uploaded_files):
    """Process uploaded PDF files and merge with existing vector store"""
    with st.spinner("PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        all_docs = []
        for uploaded_file in uploaded_files:
            # Save uploaded file to temp directory
            file_path = os.path.join(TEMP_DIR, uploaded_file.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            try:
                # Try with PyPDFLoader first
                loader = PyPDFLoader(file_path)
                documents = loader.load()
                
                # If empty, try with UnstructuredPDFLoader
                if not documents:
                    loader = UnstructuredPDFLoader(file_path)
                    documents = loader.load()
                
                all_docs.extend(documents)
                st.session_state.processed_files.append(uploaded_file.name)
            except Exception as e:
                st.error(f"'{uploaded_file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        if not all_docs:
            st.error("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return st.session_state.vectordb  # Return existing vectordb
        
        # Split documents into chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""],
        )
        texts = text_splitter.split_documents(all_docs)
        
        # Create embeddings and vector store
        embeddings = OpenAIEmbeddings(api_key=load_api_key())
        
        # If we already have a vectordb, merge the new documents
        if st.session_state.vectordb:
            vectordb = st.session_state.vectordb
            vectordb.add_documents(texts)
        else:
            vectordb = FAISS.from_documents(texts, embeddings)
        
        # Save the updated vectorstore
        vectordb.save_local(VECTOR_STORE_PATH)
        
        return vectordb

def initialize_chain(vectordb, doc_weight=0.7, model_temperature=0.7):
    """Initialize the conversation chain with the vector store and balance settings"""
    api_key = load_api_key()
    if not api_key:
        return None
    
    llm = ChatOpenAI(
        model=MODEL_NAME, 
        temperature=model_temperature,
        api_key=api_key
    )
    
    # Rewrite prompt for question improvement
    rewrite_prompt = PromptTemplate(
        input_variables=["question"],
        template="""
        ë‹¹ì‹ ì€ ì‚°ì—…ì¬í•´ ê´€ë ¨ ë²•ë¥  ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë²•ë¥  ì „ë¬¸ AIì…ë‹ˆë‹¤.
        ì›ë³¸ ì§ˆë¬¸: {question}
        
        ë²•ë¥ ì  ë§¥ë½ê³¼ ê´€ë ¨ ì¡°í•­ì„ ì°¾ê¸° ìœ„í•´ ê°œì„ ëœ ì§ˆë¬¸:
        """
    )   
    
    # QA prompt to include source documents with fixed doc_weight
    qa_prompt = PromptTemplate(
        input_variables=["context", "question"],
        template=f"""
        ë‹¹ì‹ ì€ ì‚°ì—…ì¬í•´ ê´€ë ¨ ë²•ë¥  ë¬¸ì„œì™€ ë²•ì  ì§€ì‹ì„ ê²°í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ë²•ë¥  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ë²•ë¥  ì¡°ì–¸ìì…ë‹ˆë‹¤.

        ## ë²•ë¥  ë¬¸ì„œ ë‚´ìš©:
        {{context}}

        ## ì§ˆë¬¸:
        {{question}}

        ## ë‹µë³€ ìƒì„± ì§€ì¹¨:
        - ë¬¸ì„œ ë‚´ìš© ì˜ì¡´ë„: {doc_weight*9.5}/10 (10ì´ë©´ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©, 1ì´ë©´ AI ì§€ì‹ ê±°ì˜ ì „ì ìœ¼ë¡œ ì‚¬ìš©)
        - ì‚°ì—…ì•ˆì „ë³´ê±´ë²•, ì‚°ì—…ì¬í•´ë³´ìƒë³´í—˜ë²• ë“± ê´€ë ¨ ë²•ë¥ ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•œ, ë²•ì ìœ¼ë¡œ ê±´ì „í•œ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
        - ë²•ì¡°í•­ê³¼ ê´€ë ¨ íŒë¡€ê°€ ìˆë‹¤ë©´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš” (ì˜ˆ: "ì‚°ì—…ì•ˆì „ë³´ê±´ë²• ì œ00ì¡°ì— ë”°ë¥´ë©´...")
        - ì‚¬ìš©ìê°€ ë²•ì  ì ˆì°¨, ê¶Œë¦¬, ì˜ë¬´ì— ëŒ€í•´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
        - ë³µì¡í•œ ë²•ë¥  ìš©ì–´ê°€ ìˆë‹¤ë©´ ì¼ë°˜ì¸ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”
        - ë¬¸ì„œì— íŠ¹ì • ìƒí™©ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ë‹¤ë©´ "ê·€í•˜ì˜ êµ¬ì²´ì ì¸ ìƒí™©ì— ë”°ë¼ ë‹µë³€ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´ì„ ê¶Œì¥í•©ë‹ˆë‹¤"ë¼ê³  ì–¸ê¸‰í•˜ì„¸ìš”
        - ë‹µë³€ ë§ˆì§€ë§‰ì— í•­ìƒ "ì´ ì •ë³´ëŠ” ì¼ë°˜ì ì¸ ë²•ë¥  ì •ë³´ ì œê³µ ëª©ì ì´ë©°, ê°œë³„ ì‚¬ë¡€ì— ëŒ€í•œ ë²•ë¥  ì¡°ì–¸ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•í•œ ë²•ë¥  ìë¬¸ì„ ìœ„í•´ì„œëŠ” ì‚°ì¬ ì „ë¬¸ ë³€í˜¸ì‚¬ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."ë¼ëŠ” ë©´ì±… ë¬¸êµ¬ë¥¼ í¬í•¨í•˜ì„¸ìš”
        
        ë‹µë³€:
        """
    )
    
    # Set up memory and conversation chain
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    # Improved chain with source documents
    rewrite_chain = LLMChain(llm=llm, prompt=rewrite_prompt)
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 7}),
        memory=memory,
        condense_question_prompt=rewrite_prompt,
        combine_docs_chain_kwargs={"prompt": qa_prompt}
    )
    
    st.session_state.rewrite_chain = rewrite_chain
    return conversation_chain

# Initialize session state variables
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "conversation_chain" not in st.session_state:
    st.session_state.conversation_chain = None

if "rewrite_chain" not in st.session_state:
    st.session_state.rewrite_chain = None

if "processed_files" not in st.session_state:
    st.session_state.processed_files = []

if "default_pdfs" not in st.session_state:
    st.session_state.default_pdfs = []

if "openai_api_key" not in st.session_state:
    st.session_state.openai_api_key = ""

if "doc_weight" not in st.session_state:
    st.session_state.doc_weight = 7

if "model_temperature" not in st.session_state:
    st.session_state.model_temperature = 0.7

if "vectordb" not in st.session_state:
    # Try to load existing vectorstore first
    st.session_state.vectordb = load_saved_vectorstore()
    
    # If no existing vectorstore, load default PDFs
    if st.session_state.vectordb is None:
        st.session_state.vectordb = load_default_pdfs()
        
    # Initialize conversation chain if vectordb exists
    if st.session_state.vectordb:
        st.session_state.conversation_chain = initialize_chain(
            st.session_state.vectordb,
            st.session_state.doc_weight / 10,
            st.session_state.model_temperature
        )

# UI Layout
st.title("ğŸ“„ PDF ê¸°ë°˜ GPT-4.1 ì±—ë´‡")
st.markdown("ì‚¬ì „ ë¡œë“œëœ PDF íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€í™”í•˜ê±°ë‚˜, ì¶”ê°€ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

# Sidebar for API Key and settings
with st.sidebar:
    st.header("ì„¤ì •")
    
    # API Key input
    api_key_input = st.text_input(
        "OpenAI API í‚¤",
        type="password",
        value=st.session_state.openai_api_key,
        help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”. .env íŒŒì¼ì— ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
    )
    
    if api_key_input:
        st.session_state.openai_api_key = api_key_input
    
    # Show loaded default files
    if st.session_state.default_pdfs:
        st.subheader("ê¸°ë³¸ ë¡œë“œëœ PDF íŒŒì¼")
        for file in st.session_state.default_pdfs:
            st.write(f"- {file}")
    
    # Additional file upload
    st.subheader("ì¶”ê°€ PDF íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_files = st.file_uploader(
        "PDF íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type="pdf",
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸ“„ ì¶”ê°€ PDF ì²˜ë¦¬í•˜ê¸°"):
        st.session_state.vectordb = process_pdfs(uploaded_files)
        if st.session_state.vectordb:
            st.session_state.conversation_chain = initialize_chain(
                st.session_state.vectordb, 
                st.session_state.doc_weight / 10,
                st.session_state.model_temperature
            )
            st.success(f"âœ… {len(uploaded_files)}ê°œ ì¶”ê°€ PDF íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
    
    # Show processed user files
    if st.session_state.processed_files:
        st.subheader("ì‚¬ìš©ì ì¶”ê°€ PDF íŒŒì¼")
        for file in st.session_state.processed_files:
            st.write(f"- {file}")
    
    # ë°ì´í„° ê´€ë¦¬ ì˜µì…˜
    st.divider()
    st.subheader("ë°ì´í„° ê´€ë¦¬")
    
    if st.button("ğŸ”„ ê¸°ë³¸ PDF ì¬ì²˜ë¦¬"):
        st.session_state.default_pdfs = []
        st.session_state.vectordb = load_default_pdfs()
        if st.session_state.vectordb:
            st.session_state.conversation_chain = initialize_chain(
                st.session_state.vectordb, 
                st.session_state.doc_weight / 10,
                st.session_state.model_temperature
            )
    
    if st.button("ğŸ—‘ï¸ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”", type="primary"):
        if os.path.exists(VECTOR_STORE_PATH):
            import shutil
            shutil.rmtree(VECTOR_STORE_PATH)
            os.makedirs(VECTOR_STORE_PATH, exist_ok=True)
        st.session_state.vectordb = None
        st.session_state.conversation_chain = None
        st.session_state.default_pdfs = []
        st.session_state.processed_files = []
        st.success("âœ… ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")
    
    # Response balance settings
    st.divider()
    st.subheader("ì‘ë‹µ ì„¤ì •")
    
    # Document weight slider
    doc_weight = st.slider(
        "ë¬¸ì„œ ì˜ì¡´ë„",
        min_value=1,
        max_value=10,
        value=st.session_state.doc_weight,
        help="10ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¬¸ì„œ ë‚´ìš©ì— ì¶©ì‹¤í•˜ê²Œ, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ AIì˜ ì§€ì‹ì„ ë” í™œìš©í•©ë‹ˆë‹¤."
    )
    
    # Model temperature slider
    model_temp = st.slider(
        "ì°½ì˜ì„± ìˆ˜ì¤€",
        min_value=0.0,
        max_value=1.0,
        value=st.session_state.model_temperature,
        step=0.1,
        help="ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ë‹µë³€, ë‚®ì„ìˆ˜ë¡ ë” ì¼ê´€ëœ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    )
    
    # Apply settings button
    if st.button("ì„¤ì • ì ìš©"):
        if doc_weight != st.session_state.doc_weight or model_temp != st.session_state.model_temperature:
            st.session_state.doc_weight = doc_weight
            st.session_state.model_temperature = model_temp
            
            # Reinitialize chain with new settings if vectordb exists
            if st.session_state.vectordb:
                st.session_state.conversation_chain = initialize_chain(
                    st.session_state.vectordb,
                    doc_weight / 10,
                    model_temp
                )
                st.success("âœ… ìƒˆë¡œìš´ ì„¤ì •ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # Settings info
    current_settings = f"""
    **í˜„ì¬ ì„¤ì •:**
    - ë¬¸ì„œ ì˜ì¡´ë„: {st.session_state.doc_weight}/10
    - ì°½ì˜ì„± ìˆ˜ì¤€: {st.session_state.model_temperature:.1f}
    """
    st.markdown(current_settings)
    
    # History management
    st.divider()
    st.subheader("ëŒ€í™” ê¸°ë¡ ê´€ë¦¬")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ’¾ ëŒ€í™” ì €ì¥"):
            save_chat_history(st.session_state.chat_history)
            st.success("âœ… ëŒ€í™” ì €ì¥ ì™„ë£Œ")
    
    with col2:
        if st.button("ğŸ“‚ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°"):
            st.session_state.chat_history = load_chat_history()
            st.success("âœ… ì´ì „ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.success("âœ… ëŒ€í™” ë‚´ì—­ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.")

# Main chat area
if not st.session_state.vectordb:
    st.warning("âš ï¸ 'data' ë””ë ‰í† ë¦¬ì— ê¸°ë³¸ PDF íŒŒì¼ì´ ì—†ê±°ë‚˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.")
elif not st.session_state.conversation_chain:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ 'PDF ì²˜ë¦¬í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.")
else:
    st.success(f"âœ… {len(st.session_state.default_pdfs) + len(st.session_state.processed_files)}ê°œì˜ PDF íŒŒì¼ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!")

# Display chat history
chat_container = st.container()
with chat_container:
    for user_msg, bot_msg in st.session_state.chat_history:
        with st.chat_message("user"):
            st.markdown(user_msg)
        with st.chat_message("assistant"):
            st.markdown(bot_msg)

# User input
if st.session_state.conversation_chain:
    user_question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    if user_question:
        # Display user message immediately
        with st.chat_message("user"):
            st.markdown(user_question)
        
        # Process the question and get a response
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            try:
                # Improve the question
                improved_question = st.session_state.rewrite_chain.run(user_question)
                
                # Get response with improved question and current document weight
                response = st.session_state.conversation_chain.run(improved_question)
                
                # Display assistant response
                with st.chat_message("assistant"):
                    st.markdown(response)
                
                # Add to history
                st.session_state.chat_history.append((user_question, response))
                
                # Auto-save history
                save_chat_history(st.session_state.chat_history)
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.error("ìƒì„¸ ì •ë³´: " + str(type(e).__name__) + " - " + str(e))